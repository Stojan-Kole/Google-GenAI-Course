{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2e5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neohpdono je namestiti google Api kljuc u Google AI Studio\n",
    "#https://aistudio.google.com/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d42f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API key as an environment variable\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDmBKYMTUjeYnepxaHDFNuPFk1VY4nDq_k\"\n",,
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1819a954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDmBKYMTUjeYnepxaHDFNuPFk1VY4nDq_k\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ece6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637ead96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-genai\r\n",
      "Version: 1.8.0\r\n",
      "Summary: GenAI Python SDK\r\n",
      "Home-page: \r\n",
      "Author: \r\n",
      "Author-email: Google LLC <googleapis-packages@google.com>\r\n",
      "License: Apache-2.0\r\n",
      "Location: /home/stole/.local/lib/python3.12/site-packages\r\n",
      "Requires: anyio, google-auth, httpx, pydantic, requests, typing-extensions, websockets\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd3f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb783a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e81c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google\r\n",
      "Version: 3.0.0\r\n",
      "Summary: Python bindings to the Google search engine.\r\n",
      "Home-page: http://breakingcode.wordpress.com/\r\n",
      "Author: Mario Vilas\r\n",
      "Author-email: mvilas@gmail.com\r\n",
      "License: UNKNOWN\r\n",
      "Location: /home/stole/.local/lib/python3.12/site-packages\r\n",
      "Requires: beautifulsoup4\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900e9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd11fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prikazivanje generisanja sadrzaja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af8b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a really smart robot toy. This robot can learn things!\n",
      "\n",
      "Normally, toys do only what you tell them. But AI is like giving the robot a brain that can figure things out on its own.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "*   **Learning from examples:** You show the robot lots of pictures of cats and dogs. After seeing enough, it can start to tell the difference between them, even if it's a picture it's never seen before!\n",
      "\n",
      "*   **Playing games:** You teach the robot to play a game like tic-tac-toe. It plays lots and lots of times. Each time, it learns what moves work best to win. Eventually, it becomes a really good player!\n",
      "\n",
      "*   **Understanding you:** You teach the robot to understand your voice. It learns what you mean when you say \"Turn on the lights\" or \"Play my favorite song.\"\n",
      "\n",
      "So, AI is like giving a computer (or robot) the ability to learn, understand, and solve problems like a smart person. It's like magic, but it's made with code and lots of data!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain AI to me like I'm a kid.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ebd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prikazivanje razgovora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e168b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings, Zlork! It's nice to meet you.\n",
      "\n",
      "Here's something interesting about Novi Sad:\n",
      "\n",
      "**Novi Sad is often called \"Serbian Athens\" or \"Athens of Serbia.\"** This nickname comes from its rich history as a center of Serbian culture and intellectual life, particularly during the 18th and 19th centuries when the city was a vital part of the Austro-Hungarian Empire. It fostered numerous writers, artists, and intellectuals, and was a crucial hub for the preservation and development of Serbian identity. This cultural prominence gave it a reputation similar to the birthplace of Western civilization, Athens.\n",
      "\n",
      "Is there anything else you'd like to know about Novi Sad? I can tell you about its fortress, its festival, its history, or anything else that interests you! Just let me know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('Hello! My name is Zlork. Tell me something about Novi Sad')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b38639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, Zlork! Let's compare and contrast Novi Sad and Subotica. Here's a combined overview:\n",
      "\n",
      "**Similarities between Novi Sad and Subotica:**\n",
      "\n",
      "*   **Location:** Both are located in Vojvodina, the northern province of Serbia, known for its fertile plains and rich agricultural history.\n",
      "*   **History:** Both cities have experienced periods under various empires and influences, including Austro-Hungarian and Ottoman rule, leaving a diverse architectural and cultural heritage.\n",
      "*   **Multiethnic populations:** Both cities boast a diverse population, with Serbs, Hungarians, Croats, and other ethnic groups living together. This multiethnic character contributes to a rich cultural tapestry.\n",
      "*   **Cultural centers:** Both are important cultural and educational centers in Serbia, hosting festivals, museums, theaters, and universities.\n",
      "*   **Economic importance:** Both serve as key economic hubs in Vojvodina, with industries such as agriculture, food processing, and manufacturing playing a significant role.\n",
      "*   **Art Nouveau Architecture:** Both cities have beautiful examples of Art Nouveau architecture from the Austro-Hungarian period.\n",
      "\n",
      "**Differences between Novi Sad and Subotica:**\n",
      "\n",
      "*   **Size and Importance:** Novi Sad is larger and generally considered more important economically and politically than Subotica. Novi Sad is the capital of Vojvodina, while Subotica is the administrative center of the North Bačka District.\n",
      "*   **Dominant Culture:** While both are multiethnic, Novi Sad has a stronger Serbian cultural presence, while Subotica has a more prominent Hungarian cultural influence. This is reflected in the languages spoken, cultural events, and even the architecture.\n",
      "*   **Main Attractions:**\n",
      "    *   **Novi Sad:** Petrovaradin Fortress (a massive historical fortress overlooking the Danube), the EXIT festival (one of Europe's largest music festivals), and the beautiful city center with its pedestrian zone.\n",
      "    *   **Subotica:** Subotica City Hall (an impressive example of Art Nouveau architecture), Palić Lake (a resort area with beaches and recreational facilities), and numerous religious buildings.\n",
      "*   **Vibe:** Novi Sad has a more modern and bustling feel due to its size and economic activity, while Subotica has a slightly more relaxed and traditional atmosphere.\n",
      "*   **Language:** While Serbian is the official language in both cities, Hungarian is more widely spoken and used in Subotica, with many signs and institutions offering services in Hungarian.\n",
      "\n",
      "**In a nutshell:**\n",
      "\n",
      "*   **Novi Sad:** A larger, more prominent city with a strong Serbian cultural presence, known for its fortress and music festival.\n",
      "*   **Subotica:** A charming city with a strong Hungarian influence, known for its Art Nouveau architecture and relaxed atmosphere.\n",
      "\n",
      "Hopefully, this gives you a good overview of both cities! Is there anything else you'd like to know, or a specific aspect you'd like me to elaborate on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell me something about Novi Sad and Subotica as well?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fbc48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea3d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svojstva modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "693ff253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "    if model.name == 'models/gemini-2.0-flash':\n",
    "        pprint(model.to_json_dict())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a2b28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uticaj korisnickih restrikcija na model. I ako sam mu napisao 1000 reci, nije mogao da ode preko 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78216115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Might and Majesty of Emperor Dušan: A Legacy of Law, Expansion, and Ambition in the Medieval Balkans\n",
      "\n",
      "Emperor Stefan Dušan IV Nemanjić, often referred to as Dušan the Mighty, reigned over Serbia from 1331 to 1355, a period that marks the zenith of Serbian power and cultural influence in the medieval Balkans. While his reign was relatively short, Dušan transformed Serbia from a regional power into a sprawling empire, encompassing vast territories and leaving an enduring legacy through his ambitious legal code, the Dušan's Code, and his imperial aspirations that redefined the political landscape of Southeast Europe. His story is one of relentless ambition, shrewd diplomacy, and a vision for a powerful and culturally sophisticated Serbian empire that challenged the authority of both Byzantium and the emerging Ottoman threat.\n",
      "\n",
      "Dušan's ascent to power was marked by a brutal act: the overthrow and death of his father, Stefan Dečanski. Dečanski,\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "config = types.GenerateContentConfig(max_output_tokens = 200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = config,\n",
    "    contents = \"Write me a 1000 words essay about Emperor Dusan The Mighty\"\n",
    "    )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d92ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temperatura je koncept koji kontrolise nasumicnost selekcije. Visa temperatura znaci da model bira vise kandidata\n",
    "#dok temperatura od 0 znaci da ce najocigledniji kandidat bit odabran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad70e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mauve\n",
      " -------------------------\n",
      "Mauve.\n",
      " -------------------------\n",
      "Magenta\n",
      " -------------------------\n",
      "Cerulean.\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e82df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_config = types.GenerateContentConfig(temperature=0.0)\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=low_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5eaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top-P je parametar koji takodje sluzi za raznolikolst izlaza modela. \n",
    "#Top-P definira prag verovatnoće koji, kada se kumulativno premaši, zaustavlja selektovanje tokena kao kandidata. \n",
    "#Top-P od 0 je obično ekvivalentan greedy dekodiranju, dok Top-P od 1 obično selektuje svaki token u vokabularu \n",
    "#modela.Takođe, možete sresti top-K u literaturi o LLM modelima. Top-K nije konfigurabilan u \n",
    "#Gemini 2.0 seriji modela, ali može biti promenjen u starijim modelima. Top-K je pozitivni ceo broj koji definiše \n",
    "#broj najverovatnijih tokena iz kojih se bira izlazni token. Top-K od 1 selektuje jedan token, što znači da se \n",
    "#koristi greedy dekodiranje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e9964b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the hushed alleyways of Pawtropolis, where shadows danced with moonbeams, lived a feline of peculiar talent. Midnight, a sleek black cat with eyes like emeralds, was no ordinary housecat. He was a swashbuckler, a fencer of unparalleled skill, known in the city's underbelly as \"The Shadow Blade.\"\n",
      "\n",
      "Midnight's weapon of choice was a miniature rapier, forged from a broken umbrella spoke and meticulously sharpened against the cobblestones. He'd learned his art from an old, grizzled tomcat named Scaramouche, a former circus performer with a penchant for dramatic flourishes and equally dramatic falls. Scaramouche was gone now, lost to the streets, but his teachings lived on in Midnight's lithe movements and cunning strategy.\n",
      "\n",
      "Tonight, Midnight's whiskers twitched with anticipation. He'd heard whispers of a new challenger in the fish market – a burly ginger tabby named Rusty, notorious for his brute strength and penchant for stealing salmon. Rusty had been boasting about his invincibility, a claim that ruffled Midnight's fur the wrong way.\n",
      "\n",
      "He found Rusty holding court amongst a gaggle of admiring strays, a half-eaten fish bone clenched in his teeth. \"So, you're the Shadow Blade?\" Rusty sneered, flicking his tail dismissively. \"Heard you were all fluff and no bite.\"\n",
      "\n",
      "Midnight landed silently before him, the moonlight glinting off his rapier. \"We shall see, ginger. We shall see.\"\n",
      "\n",
      "The crowd hushed as the two cats circled each other. Rusty lunged first, his massive paws swiping with clumsy force. Midnight parried with a flick of his wrist, the rapier a blur of silver. He was lighter, faster, more agile. He danced around Rusty's clumsy attacks, landing precise, stinging jabs on his flanks.\n",
      "\n",
      "Rusty roared in frustration, his attacks growing wilder. Midnight saw his chance. He feinted left, then darted right, his rapier finding its mark – a shallow scratch on Rusty's nose. The ginger tabby yowled in surprise and pain, stumbling backwards.\n",
      "\n",
      "Midnight pressed his advantage. He disarmed Rusty of the fish bone with a swift parry, then pinned him against a stack of crates. He held the rapier at Rusty's throat, a drop of blood beading on the point.\n",
      "\n",
      "\"Yield, ginger,\" Midnight hissed, his emerald eyes gleaming.\n",
      "\n",
      "Rusty, defeated and humiliated, finally muttered, \"I yield!\"\n",
      "\n",
      "Midnight lowered his rapier, sheathing it beneath his fur. He tossed the fish bone back to Rusty. \"A little skill goes a long way,\" he purred, then vanished back into the shadows, leaving Rusty to the jeers of the crowd.\n",
      "\n",
      "As he melted into the darkness, Midnight knew his reputation was secure. He was more than just a cat. He was the Shadow Blade, the protector of the alleys, the whisper of steel in the night. And he would always be ready to defend the honor of Pawtropolis, one swift parry at a time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(temperature=1.0, top_p = 0.95)\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who can swordfight\"\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = model_config,\n",
    "    contents = story_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cba384ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero-shot direktno modelu trazi zahtev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c77b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3c6eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#primer u AI STUDIO https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4979873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#moguca resenja mogu da se deklarisu preko enuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d20ea839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a64c6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.POSITIVE\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea145d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-shot i few-shot su koncepti koji daju jasan primer izlaza koji se trazi\n",
    "#https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b461048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3860ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain of thought tehnika koja daje modelu jasne medjukorake koji ga bolje privlace resenju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbb44fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "440ee072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem:\n",
       "\n",
       "1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n",
       "2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "3.  **Determine your partner's current age:** Since the age difference remains the same, your partner is currently 20 + 8 = 28 years old.\n",
       "\n",
       "**Therefore, your partner is currently 28 years old.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a298befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReAct: Reason and act omogucava da se interaguje u toku procesa izvrsavanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4576fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6bc9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and then find the youngest author listed on the paper.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "# You will perform the Action; so generate up to, but not including, the Observation.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Create a chat that has the model instructions and examples pre-seeded.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a35bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "Now I have a list of authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find their birthdates to determine the youngest. This might be difficult to determine without more information. I will start with Aidan N. Gomez, since Aidan is a relatively modern name, implying a younger author.\n",
      "\n",
      "Action 2\n",
      "<search>Aidan N. Gomez</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07f08c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training mode je eksperimenalan proces koji Geminiju omogucava da prikaze svoj tok misli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e80ef7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The youngest author listed on the groundbreaking \"Attention is All You Need\" paper (which introduced the Transformer architecture in NLP) is **Aidan N. Gomez**.\n",
       "\n",
       "At the time of publication in 2017, Aidan N. Gomez was a PhD student at the University of Toronto, while the other authors were primarily researchers at Google Brain. This generally indicates he was at an earlier stage in his career compared to the other authors, making him the youngest."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import clear_output\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model = 'gemini-2.0-flash-thinking-exp',\n",
    "    contents = 'Who was the youngest author listed on the transformers NLP paper')\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47d7dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code promting - gemini moze da generise kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2144dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"\n",
       "  Calculate the factorial of a number.\n",
       "\n",
       "  Args:\n",
       "    n: A non-negative integer.\n",
       "\n",
       "  Returns:\n",
       "    The factorial of n.\n",
       "  \"\"\"\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "470ff1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code execution - gemini moze i da izvrsi kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b28245b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Okay, I can do that. First, I'll generate the first 14 odd prime \"\n",
      "         'numbers. Remember that a prime number is a whole number greater than '\n",
      "         '1 that has only two divisors: 1 and itself. Also, the only even '\n",
      "         'prime number is 2. So, I need to find the first 14 prime numbers '\n",
      "         '*after* 2.\\n'\n",
      "         '\\n'\n",
      "         \"Then, I'll calculate the sum of those numbers.\\n\"\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'primes = []\\n'\n",
      "                             'num = 3\\n'\n",
      "                             'count = 0\\n'\n",
      "                             'while count < 14:\\n'\n",
      "                             '    is_prime = True\\n'\n",
      "                             '    for i in range(2, int(num**0.5) + 1):\\n'\n",
      "                             '        if num % i == 0:\\n'\n",
      "                             '            is_prime = False\\n'\n",
      "                             '            break\\n'\n",
      "                             '    if is_prime:\\n'\n",
      "                             '        primes.append(num)\\n'\n",
      "                             '        count += 1\\n'\n",
      "                             '    num += 2\\n'\n",
      "                             '\\n'\n",
      "                             \"print(f'{primes=}')\\n\"\n",
      "                             '\\n'\n",
      "                             'import numpy as np\\n'\n",
      "                             'sum_of_primes = np.sum(primes)\\n'\n",
      "                             \"print(f'{sum_of_primes=}')\\n\",\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
      "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "                                     '31, 37, 41, 43, 47]\\n'\n",
      "                                     'sum_of_primes=np.int64(326)\\n'}}\n",
      "-----\n",
      "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
      "         '31, 37, 41, 43, and 47.\\n'\n",
      "         '\\n'\n",
      "         'The sum of these numbers is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "config = types.GenerateContentConfig(\n",
    "    tools = [types.Tool(code_execution=types.ToolCodeExecution())])\n",
    "\n",
    "code_exec_promt = '''\n",
    "Generate the first 14 odd prime numbers, then calculate sum\n",
    "'''\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    config = config,\n",
    "    contents = code_exec_promt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07a2d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explaining code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ee10b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a Bash script designed to enhance your command-line prompt with information about the current Git repository. It's often called `bash-git-prompt` or similar.\n",
       "\n",
       "Here's a high-level breakdown:\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "*   **Displays Git Status:** It shows information like the current branch, if there are uncommitted changes, staged changes, the status of your branch compared to the remote (ahead, behind), and other relevant Git details directly in your terminal prompt.\n",
       "*   **Customizable:** It allows you to customize the appearance of the prompt through themes, colors, and symbols.  You can choose from pre-built themes or create your own.\n",
       "*   **Asynchronous Updates:** To avoid slowing down your shell, it uses asynchronous commands to fetch remote Git status, meaning it updates this information in the background without blocking your command execution.\n",
       "*   **Virtual Environment Awareness:** It can optionally display the active Python virtual environment (or similar environments like Node.js or Conda) in your prompt.\n",
       "*   **Cross-Shell Compatibility**: Aims to support both Bash and Zsh shells with specific syntax handling.\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "*   **At-a-Glance Git Information:** Quickly see the status of your Git repository without having to run `git status` every time.\n",
       "*   **Improved Workflow:** Makes it easier to keep track of your Git activities, leading to a more efficient development workflow.\n",
       "*   **Customization:** Tailor your command-line environment to your personal preferences and needs.\n",
       "*   **Visual Cues:** Use color-coding to quickly identify important Git states (e.g., uncommitted changes in red).\n",
       "\n",
       "**In essence, it transforms your command-line prompt into a dynamic Git status indicator.**  It improves the overall usability of your terminal when working with Git repositories.  You'd use it to avoid constantly typing `git status` and to have key Git information visible at all times.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=explain_prompt)\n",
    "\n",
    "Markdown(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2b4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
